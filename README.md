# changes from Aaryan-Kapoor/z-image-turbo
- FP8 default
- Select GPU option in settings
- MCP server as part of backend

# Z-Image-Turbo

> A professional web interface for the Tongyi-MAI Z-Image-Turbo model â€” lightning-fast text-to-image generation with 6B parameters.

![Z-Image-Turbo Interface](assets/projectScreenshot.png)

![Z-Image-Turbo](https://img.shields.io/badge/Model-Z--Image--Turbo-blue) ![License](https://img.shields.io/badge/License-Apache%202.0-green)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 16+
- 8GB+ VRAM recommended (or use CPU offload)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Aaryan-Kapoor/z-image-turbo.git
   cd z-image-turbo
   ```

2. **Backend Setup**
   ```bash
   python -m venv venv
   
   # Windows
   .\venv\Scripts\activate
   # Linux/Mac
   source venv/bin/activate
   
   # Standard installation
   pip install -r requirements.txt

   # OR for CUDA 12.1 (Recommended for NVIDIA GPUs)
   pip install -r requirements-cuda.txt
   ```

3. **Frontend Setup**
   ```bash
   cd frontend
   npm install
   ```

### Running the Application

**Terminal 1 - Start Backend:**
```bash
.\venv\Scripts\activate  # or source venv/bin/activate on Linux/Mac
cd backend
python main.py
```

**Terminal 2 - Start Frontend:**
```bash
cd frontend
npm run dev
```

Open **`http://localhost:5173`** in your browser and start generating!

---

## âœ¨ Features

### Application
- **Premium Dark UI** â€” Glassmorphism design with intuitive controls
- **Smart Presets** â€” Quick aspect ratios (1:1, 3:4, 16:9) and resolutions (480p-1080p)
- **Fine Control** â€” Sliders for dimensions, inference steps, guidance scale, and seed
- **Real-time Progress** â€” Live generation tracking
- **Flexible Deployment** â€” Custom model cache directory, CPU offload option
- **MCP Support** â€” Native Model Context Protocol integration for AI agents

### Model (Z-Image-Turbo)
- **âš¡ Lightning Fast** â€” Optimized for **8-step generation**, achieving sub-second latency on enterprise GPUs.
- **ğŸ—ï¸ S3-DiT Architecture** â€” Built on **Scalable Single-Stream Diffusion Transformer** technology.
- **ğŸ§  Advanced Encoders** â€” Uses **Qwen 4B** for powerful language understanding and **Flux VAE** for image decoding.
- **ğŸ“ DMDR Training** â€” Trained using **Fusing DMD with Reinforcement Learning** for superior semantic alignment.
- **ğŸŒ Bilingual Mastery** â€” Exceptional rendering of text in both **English and Chinese**.
- **ğŸ¨ Versatile & Uncensored** â€” From photorealism to anime, handling complex concepts without censorship.
- **ğŸ“ High Fidelity** â€” Native support for resolutions up to **2MP** (e.g., 1024x1536, 1440x1440).
- **ğŸ’¾ Efficient** â€” 6B parameters, comfortably fitting in 16GB VRAM (fully loaded on GPU) using default FP8 quantization.

---

## ğŸ”¬ Technical Architecture

Z-Image-Turbo represents a significant leap in efficient generative AI:

*   **Base Architecture**: S3-DiT (Scalable Single-Stream DiT)
*   **Text Encoder**: Qwen 4B (Large Language Model based conditioning)
*   **VAE**: Flux Autoencoder
*   **Training Method**: Distilled from Z-Image using DMDR (DMD + RL)
*   **Inference**: 8 NFEs (Number of Function Evaluations) default
*   **Precision**: Optimized for bfloat16 / fp8 (Default)

---

## ğŸ› ï¸ Tech Stack

- **Backend:** FastAPI, PyTorch, Diffusers, Transformers
- **Frontend:** React, Vite, Lucide React
- **Model:** Tongyi-MAI/Z-Image-Turbo (6B parameters)

---

## âš™ï¸ Configuration

Access settings via the gear icon in the sidebar:
- **Model Cache Directory** â€” Specify where to download/store the model
- **CPU Offload** â€” Enable for GPUs with limited VRAM

---

## ğŸ“ License

This project is open-source under the Apache 2.0 License.

---

## ğŸ™ Credits

- **Model:** [Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) by Alibaba Group
- **UI Framework:** React + Vite
- **Backend:** FastAPI + Diffusers

## ğŸ”Œ Model Context Protocol (MCP)

This server implements the [Model Context Protocol](https://modelcontextprotocol.io/), allowing AI agents (like Claude Desktop or other MCP clients) to directly control the image generation.

- **Endpoint**: `http://localhost:8000/mcp` (Streamable HTTP)
- **Tools**:
  - `generate-image`: Generates an image from a text prompt.
    - Arguments: `prompt` (string), `height` (int), `width` (int), `steps` (int), `guidance_scale` (float), `seed` (int)

